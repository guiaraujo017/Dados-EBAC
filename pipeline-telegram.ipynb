{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Pipeline Telegram\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/pipeline_aws_chats.drawio.svg?raw=true)\n\nProjeto por [Guilherme Araújo Vasconcelos](www.linkedin.com/in/guilherme-a-vasconcelos)","metadata":{}},{"cell_type":"markdown","source":"**Objetivo**\n\nVou demonstrar como é feita a extração de dados de conversas do Telegram, fazer a transferência para um datalake, processar e analisar todos os dados extraídos. Ou seja, estamos fazendo o Pipeline , que é a automatização de processos de acordo com eventos pré especificados. \n\nComercialmente esse tipo de análise pode ser usada para identificar gargalos em Bots de suporte, até mesmo fazer uma automação do suporte, de empresas por exemplo - qual o tipo de erro mais recorrente com um produto ou serviço. ","metadata":{}},{"cell_type":"markdown","source":"# Sistemas\n\n**Transacional**\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/transacional.png?raw=true)\n\n\n\n\n\n**Analítico**\n\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/analitico.png?raw=true)\n\n\n\n\n\n**Tabela de diferença**\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/tabela%20diferenca.png?raw=true)\n\n\n\nOs dois sistemas se complementam no ETL(Extração, Transformação e Carga), como veremos a seguir.","metadata":{}},{"cell_type":"markdown","source":"# Telegram\n\n**Criação do Chatbot**\n\nPara nosso projeto foi necessário criar um bot no telegram, adicionar a um grupo e consumimos o conteúdo com o API de Bots do Telegram. \n\nO Bot foi criando abrindo um chat com o BotFather no próprio telegram:\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/criar%20bot.png?raw=true)\n\nCom o Bot criado foi necessário seguir alguns passos: \n\n*Desativar a opção de adicionar o bot em outros grupos*\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/bot%20sem%20outros%20grupos.png?raw=true)\n\n\n\n*Adicionar o Bot ao grupo*\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/grupo%20bot.png?raw=true)\n\n\n\n*Colocar o Bot como ADM do grupo*\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/bot%20adm.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"**Bot API**\n\nPara acessar as mensagens é necessário conectar o Bot com a API, usei o Google Collab. \n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/api%201.png?raw=true)\n\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/api%202.png?raw=true)\n\n\n\n\n\n\n\n*O método getMe retorna informações sobre o bot.*\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/api%203.png?raw=true)\n\n\n\n\n*O método getMe retorna as mensagens captadas pelo bot.*\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/api%204.png?raw=true)\n\n\n\n\n\n*Criação API Gateway AWS*\n\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/api%205.png?raw=true)\n","metadata":{}},{"cell_type":"markdown","source":"# Função Lambda\n\nEssa função Lambda é que vai ingerir todos os dados do grupo em um Bucket S3 de dados crus.\n\n","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport logging\nfrom datetime import datetime, timezone, timedelta\n\nimport boto3\n\n\ndef lambda_handler(event: dict, context: dict) -> dict:\n\n  '''\n  Recebe uma mensagens do Telegram via AWS API Gateway, verifica no\n  seu conteúdo se foi produzida em um determinado grupo e a escreve, \n  em seu formato original JSON, em um bucket do AWS S3.\n  '''\n\n  # vars de ambiente\n\n  BUCKET = os.environ['AWS_S3_BUCKET']\n  TELEGRAM_CHAT_ID = int(os.environ['TELEGRAM_CHAT_ID'])\n\n  # vars lógicas\n\n  tzinfo = timezone(offset=timedelta(hours=-3))\n  date = datetime.now(tzinfo).strftime('%Y-%m-%d')\n  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n\n  filename = f'{timestamp}.json'\n\n  # código principal\n\n  client = boto3.client('s3')\n  \n  try:\n\n    message = json.loads(event[\"body\"])\n    chat_id = message[\"message\"][\"chat\"][\"id\"]\n\n    if chat_id == TELEGRAM_CHAT_ID:\n\n      with open(f\"/tmp/{filename}\", mode='w', encoding='utf8') as fp:\n        json.dump(message, fp)\n\n      client.upload_file(f'/tmp/{filename}', BUCKET, f'telegram/context_date={date}/{filename}')\n\n  except Exception as exc:\n      logging.error(msg=exc)\n      return dict(statusCode=\"500\")\n\n  else:\n      return dict(statusCode=\"200\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n*Bucket S3*\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/s3%20raw.png?raw=true)\n\n\n\n\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/s3%20raw%202.png?raw=true)\n\n\n\n\n\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/s3%20raw%203.png?raw=true)\n\n\n\n\n\n\n\n\n\n\n\nComo podemos ver, todas as mensagens que o bucket recebe vem em formato JSON. \n\n\n**Webhook**\n\nA configuração do Webhook, com a URL disponibilizada pelo API Gateway, vai redirecionar as mensagens para o AWS. \n\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/webhook.png?raw=true)\n\n\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/webhook%202.png?raw=true)\n\n\n\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/webhook%203.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"# Transformação\n\nAgora que a parte de ingestão está configurada e funcionando, vamos iniciar a transformação dos dados recebidos. Novamente um lambda que fará a transformação dos dados para um novo bucket S3.\n\n\n***Bucket enriquecido S3***\n\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/enriched%201.png?raw=true)\n\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/enriched%202.png?raw=true)\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/enriched%203.png?raw=true)\n\n\n\n\nDessa vez o conteúdo salvo no bucket é no formato Parquet, que faremos a análise no AWS Athena","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport logging\nfrom datetime import datetime, timedelta, timezone\n\nimport boto3\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n\ndef lambda_handler(event: dict, context: dict) -> bool:\n\n  '''\n  Diariamente é executado para compactar as diversas mensagensm, no formato\n  JSON, do dia anterior, armazenadas no bucket de dados cru, em um único\n  arquivo no formato PARQUET, armazenando-o no bucket de dados enriquecidos\n  '''\n\n  # vars de ambiente\n\n  RAW_BUCKET = os.environ['AWS_S3_BUCKET']\n  ENRICHED_BUCKET = os.environ['AWS_S3_ENRICHED']\n\n  # vars lógicas\n\n  tzinfo = timezone(offset=timedelta(hours=-3))\n  date = (datetime.now(tzinfo) - timedelta(days=1)).strftime('%Y-%m-%d')\n  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n\n  # código principal\n\n  table = None\n  client = boto3.client('s3')\n\n  try:\n\n      response = client.list_objects_v2(Bucket=RAW_BUCKET, Prefix=f'telegram/context_date={date}')\n\n      for content in response['Contents']:\n\n        key = content['Key']\n        client.download_file(RAW_BUCKET, key, f\"/tmp/{key.split('/')[-1]}\")\n\n        with open(f\"/tmp/{key.split('/')[-1]}\", mode='r', encoding='utf8') as fp:\n\n          data = json.load(fp)\n          data = data[\"message\"]\n\n        parsed_data = parse_data(data=data)\n        iter_table = pa.Table.from_pydict(mapping=parsed_data)\n\n        if table:\n\n          table = pa.concat_tables([table, iter_table])\n\n        else:\n\n          table = iter_table\n          iter_table = None\n\n      pq.write_table(table=table, where=f'/tmp/{timestamp}.parquet')\n      client.upload_file(f\"/tmp/{timestamp}.parquet\", ENRICHED_BUCKET, f\"telegram/context_date={date}/{timestamp}.parquet\")\n\n      return True\n\n  except Exception as exc:\n      logging.error(msg=exc)\n      return False\n\ndef parse_data(data: dict) -> dict:\n\n  date = datetime.now().strftime('%Y-%m-%d')\n  timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n  parsed_data = dict()\n\n  for key, value in data.items():\n\n      if key == 'from':\n          for k, v in data[key].items():\n              if k in ['id', 'is_bot', 'first_name']:\n                parsed_data[f\"{key if key == 'chat' else 'user'}_{k}\"] = [v]\n\n      elif key == 'chat':\n          for k, v in data[key].items():\n              if k in ['id', 'type']:\n                parsed_data[f\"{key if key == 'chat' else 'user'}_{k}\"] = [v]\n\n      elif key in ['message_id', 'date', 'text']:\n          parsed_data[key] = [value]\n\n  if not 'text' in parsed_data.keys():\n    parsed_data['text'] = [None]\n\n  return parsed_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Foi necessário configurar variáveis de ambiente para o Lambda extrair o conteúdo do bucket cru e enviar o dado transformado para o bucket enriquecido**\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/ambiente.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"***EventBridge***\n\nPor fim para automatizar todo o processo, foi criado um cronograma no AWS EventBridge para que o Lambda seja executado automaticamente uma vez por dia. \n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/event.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"# Apresentação\n\n\nPor fim para analisarmos os dados, automatizados, extraídos vamos utilizar o AWS Athena para carregar os arquivos Parquet que foram gerados para então analisarmos o conteúdo. \n\n\n","metadata":{}},{"cell_type":"code","source":"CREATE EXTERNAL TABLE `telegram`(\n  `message_id` bigint,\n  `user_id` bigint,\n  `user_is_bot` boolean,\n  `user_first_name` string,\n  `chat_id` bigint,\n  `chat_type` string,\n  `text` string,\n  `date` bigint)\nPARTITIONED BY (\n  `context_date` date)\nROW FORMAT SERDE\n  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS INPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\nOUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nLOCATION\n  's3://ultimo-modulo43-datalake-enriched/telegram/'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**É necessário carregar as partições de todas as tabelas geradas que possuam partições**\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/particao.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"**Vamos verificar se a tabela foi criada corretamente**\n\nSELECT * FROM \"default\".\"telegram\" limit 10;\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/10%20primeiros.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"*Agora que foi possível confirmar que a tabela foi criada com sucesso, vamos continuar fazendo algumas consultas no SQL junto com gráficos gerados pelo CSV do resultado*\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/sql.png?raw=true)\n\n\nO número de mensagens é grande demais para caber na mesma tela, contuo já podemos verificar a extistência de outras mensagens, inclusive de outro usuário. No total foram 60 mensagens enviadas.","metadata":{}},{"cell_type":"markdown","source":"*Vamos checar o número de mensagens por dia*\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/sql%202.png?raw=true)\n\n\n\nO dia com mais mensagens foi o dia 20/11/2014\n\n\nGráfico do dia com mais mensagem:\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/msgdia.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"*Agora a quantidade de mensagens por usuário por dia.*\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/sql%203.png?raw=true)\n\n\nTanto no dia 26 quanto no dia 27 o usuário Guilherme, eu, foi quem mais mandou mensagens no grupo. \n\nGráfico de quem enviou mais mensagens no grupo:\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/qtdmsg.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"*Vamos verificar qual mensagem foi dita mais vezes*\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/sql%206.png?raw=true)\n\n\nAs mensagens \"teste\", \"tudo bem?\" e \"bom dia\" foram as mais enviadas ","metadata":{}},{"cell_type":"code","source":"# devido ao zoom para caber todo o conteúdo na tela o código ficou pequeno:\n\nselect text, count(1) as \"text_amount\"\nfrom \"telegram\"\ngroup by text\norder by text_amount desc ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Graficamente assim ficou a distribuição das mensagens\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/grafico%20text%20amount.png?raw=true)\n\n\nApenas uma observação, apesar da mensagem \"e o nosso botafogo\", **não sou botafoguense.**","metadata":{}},{"cell_type":"markdown","source":"*A seguida a média do tamanho das mensagens por usuário por dia.*\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/sql%204.png?raw=true)\n\n\n\n\nNos dias 26 e 27 o usuário I foi quem teve as maiores médias de mensagens nesses dias. \n\nGráfico média do tamanho da mensagem por usuário:\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/tamanho.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"*Agora a quantidade de mensagens por hora por dia da semana por número da semana.*\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/sql%205.png?raw=true)\n\n\n\nOs dias da semana com mais mensagens foram Terça Feira e Quarta Feira. Um dado importante é o período de maior atividade foi quase sempre no período da tarde. Graficamente falando talvez seja mais fácil visualizar:\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/datehour.png?raw=true)\n\n\n\n![](https://github.com/guiaraujo017/ImagensProjetoFinal/blob/main/weekdaymessage.png?raw=true)","metadata":{}},{"cell_type":"code","source":"# Como essa consulta SQL é maior que as outras, vou colocar o código abaixo:\n\n\nWITH\nparsed_date_cte AS (\n    SELECT\n        *,\n        CAST(date_format(from_unixtime(\"date\"),'%Y-%m-%d %H:%i:%s') AS timestamp) AS parsed_date\n    FROM \"telegram\"\n),\nhour_week_cte AS (\n    SELECT\n        *,\n        EXTRACT(hour FROM parsed_date) AS parsed_date_hour,\n        EXTRACT(dow FROM parsed_date) AS parsed_date_weekday,\n        EXTRACT(week FROM parsed_date) AS parsed_date_weeknum\n    FROM parsed_date_cte\n)\nSELECT\n    parsed_date_hour,\n    parsed_date_weekday,\n    parsed_date_weeknum,\n    count(1) AS \"message_amount\"\nFROM hour_week_cte\nGROUP BY\n    parsed_date_hour,\n    parsed_date_weekday,\n    parsed_date_weeknum\nORDER BY\n    parsed_date_weeknum,\n    parsed_date_weekday","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusão \n\n\nPodemos observar qão poderoso é um Pipelines de dados de um Chat Bot, no nosso caso Telegram. Com ele foi possível analisarmos qual mensagem apareceu mais vezes, quem falou mais e qual período/dia/semana teve mais mensagens.\n\nEssas informações, no nosso caso de estudo, podem parecer inofensivas, contudo para uma empresa é uma arma poderosíssima tanto para cortar custos quanto para aumentar a receita, como?\n\nBem, de início a Empresa X pode criar um Chat Bot com respostas automáticas para as dúvidas e solicitações mais recorrentes, como solicitar o boleto de pagamento por exemplo. A criação desse Pipeline nos daria meios de verificar as principais dúvidas e solicitações para que então fosse configurado um Bot para responder essas questões. Consequentemente quanto mais demandas forem resolvidas pelo Bot, menor é o custo de contratação com pessoal para resolver demandas. \n\nOutro ponto seria aumentar as receitas mirando clientes mais ativos em alguma plataforma, como forma de recompensa, por exemplo, poderia ser disponibilizado alguma oferta especial para algum horário ou dia que, historicamente, tem menos movimento. Nós vimos que o Pipeline foi capaz de captar essas interações de quem interage mais e até mesmo o período de interação.\n\n\ra.","metadata":{}}]}